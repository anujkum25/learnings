{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_time = pd.read_csv(\"/workspaces/learnings/Data/data_actor_critic/process_time_matrix.csv\")\n",
    "work_order = pd.read_csv(\"/workspaces/learnings/Data/data_actor_critic/work_order.csv\", names=[\"task_item\",\"starttime\",\"task\",\"maxtime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>999999</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>999999</td>\n",
       "      <td>33</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>999999</td>\n",
       "      <td>59</td>\n",
       "      <td>999999</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>999999</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>178</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>65</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>80</td>\n",
       "      <td>52</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>134</td>\n",
       "      <td>297</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>184</td>\n",
       "      <td>999999</td>\n",
       "      <td>153</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>241</td>\n",
       "      <td>136</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>999999</td>\n",
       "      <td>7</td>\n",
       "      <td>620</td>\n",
       "      <td>999999</td>\n",
       "      <td>35</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>15</td>\n",
       "      <td>1058</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       9  \\\n",
       "0  1  999999      49      41  999999      33  999999  999999  999999  999999   \n",
       "1  2  999999  999999  999999  999999  999999  999999  999999  999999  999999   \n",
       "2  3  999999  999999  999999  999999     168       6  999999  999999  999999   \n",
       "3  4  999999  999999  999999  999999     134     297  999999  999999  999999   \n",
       "4  5  999999       7     620  999999      35  999999  999999  999999     113   \n",
       "\n",
       "   ...      98      99     100     101     102     103     104     105  \\\n",
       "0  ...  999999      21      29  999999      59  999999      10      46   \n",
       "1  ...  999999     178  999999  999999  999999  999999  999999  999999   \n",
       "2  ...  999999  999999  999999      65  999999  999999  999999      80   \n",
       "3  ...  999999     184  999999     153  999999  999999  999999     241   \n",
       "4  ...  999999      15    1058  999999  999999  999999  999999      13   \n",
       "\n",
       "      106     107  \n",
       "0  999999     117  \n",
       "1  999999  999999  \n",
       "2      52     288  \n",
       "3     136     296  \n",
       "4      28       6  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_item</th>\n",
       "      <th>starttime</th>\n",
       "      <th>task</th>\n",
       "      <th>maxtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>482</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>484</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>486</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>486</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task_item  starttime  task  maxtime\n",
       "0          1        481     2       45\n",
       "1          2        482    33       45\n",
       "2          3        484     2       45\n",
       "3          4        486    10       45\n",
       "4          5        486     2       45"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>task_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     task  task_item\n",
       "0       1       1014\n",
       "1       2        877\n",
       "2       3        513\n",
       "3       4        473\n",
       "4       5        405\n",
       "..    ...        ...\n",
       "102   103         17\n",
       "103   104         32\n",
       "104   105         16\n",
       "105   106         16\n",
       "106   107         13\n",
       "\n",
       "[107 rows x 2 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(work_order.groupby('task').task_item.count()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_wrap(np_array, dtype=np.float32):\n",
    "    if np_array.dtype != dtype:\n",
    "        np_array = np_array.astype(dtype)\n",
    "    return torch.from_numpy(np_array)\n",
    "\n",
    "\n",
    "def normalized_columns_initializer(weights, std=1.0):\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True))\n",
    "    return out\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job_VM_env():\n",
    "    path = '/workspaces/learnings/Data/data_actor_critic/'\n",
    "    VM_task = pd.read_csv(path + 'process_time_matrix.csv',header=None).drop([0]).values\n",
    "    task = pd.read_csv(path + 'work_order.csv',header=None).values\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_cluster = self.VM_task.shape[1]\n",
    "        self.VM = self.VM_task.shape[0]\n",
    "        self.task_num = self.task.shape[0]\n",
    "        self.process_time = self.VM_task\n",
    "        self.VM_status = np.repeat(0,self.VM) \n",
    "        self.VM_process_task = [[] for i in range(self.VM)]\n",
    "        self.VM_process_time = [[] for i in range(self.VM)]\n",
    "        self.task_waiting_time = [[] for i in range(self.VM)]\n",
    "        self.left_task = self.task.shape[0]\n",
    "        self.done = False\n",
    "        self.total_time = 0  \n",
    "        self.task_distribute_time = np.repeat(0,self.task.shape[0])\n",
    "        self.total_task_process_time = np.repeat(0,self.task.shape[0])\n",
    "        self.task_status = np.repeat(1,self.task.shape[0])  \n",
    "        self.task_index = list(range(self.task.shape[0]))  \n",
    "        self.timeindex = 0   \n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        self.done_task = [] \n",
    "        self.done_VM = [] \n",
    "        self.task_start_time = [] \n",
    "        self.state_dim = self.state.shape[0]\n",
    "        self.action_dim = 2\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.task_num = self.task.shape[0]\n",
    "        self.VM_status = np.repeat(0,self.VM) \n",
    "        self.VM_process_task = [[] for i in range(self.VM)]\n",
    "        self.VM_process_time = [[] for i in range(self.VM)]\n",
    "        self.task_waiting_time = [[] for i in range(self.VM)]\n",
    "        #self.left_task = self.task.shape[0]\n",
    "        self.done = False\n",
    "        self.total_time = 0  \n",
    "        self.task_distribute_time = np.repeat(0,self.task.shape[0])\n",
    "        self.total_task_process_time = np.repeat(0,self.task.shape[0])\n",
    "        self.task_status = np.repeat(1,self.task.shape[0])  \n",
    "        self.task_index = list(range(self.task.shape[0]))  \n",
    "        #self.timeindex = 0  \n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        self.done_task = []\n",
    "        self.done_VM = [] \n",
    "        self.task_start_time = [] \n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        task_id = np.random.choice(a=self.task_num, size=self.VM, replace=False, p=None)\n",
    "        for i in task_id:\n",
    "            if len(self.task_index) != 0:\n",
    "                if i in self.task_index:\n",
    "                    self.task_distribute_time[i] += 1\n",
    "                    ## if more than 2, delete this task\n",
    "                    #if self.task_distribute_time[i] >= 2:\n",
    "                    #    del self.task_index[self.task_index.index(i)]\n",
    "                else:\n",
    "                    task_id[task_id.tolist().index(i)] = random.sample(self.task_index,1)[0]\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        assert action.shape[0] == self.VM\n",
    "        \n",
    "        for i in range(self.VM):\n",
    "            ## only process those tasks that are in task_index\n",
    "            if task_id[i] in self.task_index:\n",
    "                ## action = 0 indicates do not give tasks to the VM\n",
    "                if action[i] == 0 or self.VM_status[i] == 3:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.VM_process_task[i].append(task_id[i])\n",
    "                    self.VM_status[i] += 1\n",
    "                    self.task_status[task_id[i]] = 0\n",
    "                    self.VM_process_time[i].append(0)\n",
    "                    # how much time a task wait before processing\n",
    "                    self.task_waiting_time[i].append(self.timeindex)\n",
    "                    # if VM could not handle the task, exit\n",
    "                    self.total_task_process_time[task_id[i]] = self.process_time[i][self.task[task_id[i]][2]]\n",
    "                \n",
    "                delete_index = []\n",
    "                for j in range(len(self.VM_process_time[i])):\n",
    "                    if len(self.VM_process_task[i]) != 0:\n",
    "                        if self.VM_process_time[i][j] == self.process_time[i][self.task[self.VM_process_task[i][j]][2]]:\n",
    "                            # if task finished, workload of VM would decrease\n",
    "                            self.VM_status[i] -= 1\n",
    "                            self.done_VM.append(i)\n",
    "                            if self.VM_process_task[i][j] not in self.done_task:\n",
    "                                self.left_task -= 1\n",
    "                            self.done_task.append(self.VM_process_task[i][j])\n",
    "                            ## calculate when the task starts to be processed by subtracting the process time\n",
    "                            self.task_start_time.append(self.task_waiting_time[i][j] + self.task[self.VM_process_task[i][j]][1])\n",
    "                            delete_index.append(j)\n",
    "                if len(delete_index) > 0:\n",
    "                    if len(delete_index) > 1:\n",
    "                        delete_index.sort(reverse = True)\n",
    "                    for k in delete_index:\n",
    "                        del self.VM_process_task[i][k]\n",
    "                        del self.VM_process_time[i][k]\n",
    "            ## calculate total time consumed\n",
    "            self.total_time += sum(self.task_waiting_time[i]) + self.total_task_process_time[i].sum()\n",
    "            self.VM_process_time[i] = [m + 1 for m in self.VM_process_time[i]]\n",
    "        \n",
    "        ## reward takes the minus of total time*0.001 and left task num\n",
    "        #print(self.total_time)\n",
    "        reward = 1 - self.left_task/self.task_num\n",
    "        self.timeindex += 1\n",
    "        \n",
    "        ## update state info\n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        \n",
    "        if self.left_task == 0:\n",
    "            self.done = True\n",
    "        #print(self.VM_status)\n",
    "        #print(self.VM_process_task)\n",
    "        #print(self.done_task)\n",
    "        return self.state, reward, self.done, self.done_task, self.done_VM, self.task_start_time\n",
    "\n",
    "    def update(self,delete_list):\n",
    "        if len(delete_list) != 0:\n",
    "            for i in delete_list:\n",
    "                if i in self.task_index:\n",
    "                    self.task_index.remove(i)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(32*553, 256)\n",
    "\n",
    "        num_outputs = action_space\n",
    "        self.critic_linear = nn.Linear(256, 1)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = normalized_columns_initializer(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = normalized_columns_initializer(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view(-1, 32*553)\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n",
    "    \n",
    "    def choose_action(self,inputs,action_dim):\n",
    "        s, (hx, cx) = inputs\n",
    "        value, logit, (hx, cx) = self.forward((s.unsqueeze(0),(hx, cx)))\n",
    "        prob = F.softmax(logit, dim=-1)\n",
    "        log_prob = F.log_softmax(logit, dim=-1)\n",
    "        entropy = -(log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        #action = prob.multinomial(num_samples=action_dim).detach()\n",
    "        action=[]\n",
    "        for i in range(action_dim):\n",
    "            action.append(prob.multinomial(num_samples=1).detach()[0])\n",
    "        action = torch.from_numpy(np.array(action,dtype = np.int64).reshape(1,133))\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "\n",
    "    lr = 0.01\n",
    "    gamma = 0.9\n",
    "    gae_lambda = 1.00\n",
    "    entropy_coef = 0.01\n",
    "    value_loss_coef = 0.5\n",
    "    max_grad_norm = 50\n",
    "    seed= 2020\n",
    "    num_steps = 100\n",
    "    max_episode_length= 10\n",
    "    episode = 10\n",
    "\n",
    "    #torch.manual_seed(args.seed)\n",
    "\n",
    "    env = Job_VM_env()\n",
    "    \n",
    "    model = ActorCritic(env.state_dim, env.action_dim)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    state = env.reset()\n",
    "    state = v_wrap(state)\n",
    "    done = True\n",
    "    action_dim = env.VM\n",
    "\n",
    "    episode_length = 0\n",
    "    complete_tasks = []\n",
    "    VM_complete_task = []\n",
    "    complete_task_start_time = []\n",
    "    update_list = []\n",
    "    total_rewards =[]\n",
    "    for episode in range(episode):\n",
    "        \n",
    "        if done:\n",
    "            cx = torch.zeros(1, 256)\n",
    "            hx = torch.zeros(1, 256)\n",
    "        else:\n",
    "            cx = cx.detach()\n",
    "            hx = hx.detach()\n",
    "        \n",
    "        if len(complete_tasks) != 0:\n",
    "            update_list = [n for m in complete_tasks for n in m]\n",
    "            env.update(update_list)\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "\n",
    "        for step in range(num_steps+1):\n",
    "            episode_length += 1\n",
    "            \n",
    "\n",
    "            action, log_prob, entropy, value = model.choose_action((state, (hx,cx)),action_dim)\n",
    "            #print(\"choosen action:\", action, \"with Prob:\", log_prob, \"value:\", value)\n",
    "            log_prob = log_prob.gather(1, action)[0]\n",
    "            \n",
    "            state, reward, done, done_task, done_VM, task_start_time = env.step(action.view(-1,).numpy())\n",
    "            #print(\"state:\", state, \"reward at state action:\", reward, \"done_task\", done_task, \"done_VM\", done_VM)\n",
    "\n",
    "            done = done or episode_length >= max_episode_length\n",
    "            ## reward shaping\n",
    "            reward = np.round(max(min(reward, 1), -1),3)\n",
    "            if episode_length % 20 == 0:\n",
    "                print(\"reward:\", reward)\n",
    "                #print(done_task)\n",
    "\n",
    "            if done:\n",
    "                complete_tasks.append(done_task)\n",
    "                VM_complete_task.append(done_VM)\n",
    "                complete_task_start_time.append(task_start_time)\n",
    "                #print('Complete these tasks with 100 iterations:')\n",
    "                #print(complete_tasks)\n",
    "                print('Current episode:',episode)\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "                \n",
    "                final_output =[]\n",
    "                for i in range(len(complete_tasks)):\n",
    "                    for j in range(len(complete_tasks[i])):\n",
    "                        output = ([complete_tasks[i][j]+1, VM_complete_task[i][j]+1, complete_task_start_time[i][j]])\n",
    "                        final_output.append(output)\n",
    "\n",
    "                final_output1 = pd.DataFrame(final_output)\n",
    "                print(\"makespan:\",final_output1[2].sum())\n",
    "                final_output1.to_csv(\"output_file.csv\")\n",
    "\n",
    "\n",
    "            state = v_wrap(state)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            print(\"total_reward:\",rewards)\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n",
    "            R = value.detach()\n",
    "\n",
    "        values.append(R)\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            # Generalized Advantage Estimation\n",
    "            delta_t = rewards[i] + gamma * \\\n",
    "                values[i + 1] - values[i]\n",
    "            gae = gae * gamma * gae_lambda + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                log_probs[i] * gae.detach() - entropy_coef * entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (policy_loss + value_loss_coef * value_loss).backward(torch.ones_like(policy_loss))\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        print(\"total loss (policy+value):\", policy_loss.mean() + value_loss_coef * value_loss)\n",
    "        print('parameters of model updated')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: [0.0]\n",
      "total_reward: [0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Current episode: 0\n",
      "makespan: 1927\n",
      "total loss (policy+value): tensor([[-0.1286]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.0]\n",
      "total_reward: [0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Current episode: 1\n",
      "makespan: 4298\n",
      "total loss (policy+value): tensor([[54.1696]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.001]\n",
      "total_reward: [0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "Current episode: 2\n",
      "makespan: 6034\n",
      "total loss (policy+value): tensor([[-1.1183]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.001]\n",
      "total_reward: [0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "Current episode: 3\n",
      "makespan: 8900\n",
      "total loss (policy+value): tensor([[9.9707]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.001]\n",
      "total_reward: [0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "Current episode: 4\n",
      "makespan: 11606\n",
      "total loss (policy+value): tensor([[4.5965]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.001]\n",
      "total_reward: [0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002]\n",
      "total_reward: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002]\n",
      "Current episode: 5\n",
      "makespan: 15285\n",
      "total loss (policy+value): tensor([[0.6173]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.002]\n",
      "total_reward: [0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "Current episode: 6\n",
      "makespan: 15925\n",
      "total loss (policy+value): tensor([[-0.3543]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.002]\n",
      "total_reward: [0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "Current episode: 7\n",
      "makespan: 16791\n",
      "total loss (policy+value): tensor([[0.5296]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.002]\n",
      "total_reward: [0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "Current episode: 8\n",
      "makespan: 16791\n",
      "total loss (policy+value): tensor([[0.2501]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "total_reward: [0.002]\n",
      "total_reward: [0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "total_reward: [0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002]\n",
      "Current episode: 9\n",
      "makespan: 18492\n",
      "total loss (policy+value): tensor([[-0.0441]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
