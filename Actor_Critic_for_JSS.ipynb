{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_time = pd.read_csv(\"/workspaces/learnings/Data/data_actor_critic/process_time_matrix.csv\")\n",
    "work_order = pd.read_csv(\"/workspaces/learnings/Data/data_actor_critic/work_order.csv\", names=[\"task_item\",\"starttime\",\"task\",\"maxtime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>999999</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>999999</td>\n",
       "      <td>33</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>999999</td>\n",
       "      <td>59</td>\n",
       "      <td>999999</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>999999</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>178</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>65</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>80</td>\n",
       "      <td>52</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>134</td>\n",
       "      <td>297</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>184</td>\n",
       "      <td>999999</td>\n",
       "      <td>153</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>241</td>\n",
       "      <td>136</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>999999</td>\n",
       "      <td>7</td>\n",
       "      <td>620</td>\n",
       "      <td>999999</td>\n",
       "      <td>35</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>999999</td>\n",
       "      <td>15</td>\n",
       "      <td>1058</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>999999</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       9  \\\n",
       "0  1  999999      49      41  999999      33  999999  999999  999999  999999   \n",
       "1  2  999999  999999  999999  999999  999999  999999  999999  999999  999999   \n",
       "2  3  999999  999999  999999  999999     168       6  999999  999999  999999   \n",
       "3  4  999999  999999  999999  999999     134     297  999999  999999  999999   \n",
       "4  5  999999       7     620  999999      35  999999  999999  999999     113   \n",
       "\n",
       "   ...      98      99     100     101     102     103     104     105  \\\n",
       "0  ...  999999      21      29  999999      59  999999      10      46   \n",
       "1  ...  999999     178  999999  999999  999999  999999  999999  999999   \n",
       "2  ...  999999  999999  999999      65  999999  999999  999999      80   \n",
       "3  ...  999999     184  999999     153  999999  999999  999999     241   \n",
       "4  ...  999999      15    1058  999999  999999  999999  999999      13   \n",
       "\n",
       "      106     107  \n",
       "0  999999     117  \n",
       "1  999999  999999  \n",
       "2      52     288  \n",
       "3     136     296  \n",
       "4      28       6  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_item</th>\n",
       "      <th>starttime</th>\n",
       "      <th>task</th>\n",
       "      <th>maxtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>481</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>482</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>484</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>486</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>486</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task_item  starttime  task  maxtime\n",
       "0          1        481     2       45\n",
       "1          2        482    33       45\n",
       "2          3        484     2       45\n",
       "3          4        486    10       45\n",
       "4          5        486     2       45"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_order.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>task_item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     task  task_item\n",
       "0       1       1014\n",
       "1       2        877\n",
       "2       3        513\n",
       "3       4        473\n",
       "4       5        405\n",
       "..    ...        ...\n",
       "102   103         17\n",
       "103   104         32\n",
       "104   105         16\n",
       "105   106         16\n",
       "106   107         13\n",
       "\n",
       "[107 rows x 2 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(work_order.groupby('task').task_item.count()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_wrap(np_array, dtype=np.float32):\n",
    "    if np_array.dtype != dtype:\n",
    "        np_array = np_array.astype(dtype)\n",
    "    return torch.from_numpy(np_array)\n",
    "\n",
    "\n",
    "def normalized_columns_initializer(weights, std=1.0):\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True))\n",
    "    return out\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job_VM_env():\n",
    "    path = '/workspaces/learnings/Data/data_actor_critic/'\n",
    "    VM_task = pd.read_csv(path + 'process_time_matrix.csv',header=None).drop([0]).values\n",
    "    task = pd.read_csv(path + 'work_order.csv',header=None).values\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.task_cluster = self.VM_task.shape[1]\n",
    "        self.VM = self.VM_task.shape[0]\n",
    "        self.task_num = self.task.shape[0]\n",
    "        self.process_time = self.VM_task\n",
    "        self.VM_status = np.repeat(0,self.VM) \n",
    "        self.VM_process_task = [[] for i in range(self.VM)]\n",
    "        self.VM_process_time = [[] for i in range(self.VM)]\n",
    "        self.task_waiting_time = [[] for i in range(self.VM)]\n",
    "        self.left_task = self.task.shape[0]\n",
    "        self.done = False\n",
    "        self.total_time = 0  \n",
    "        self.task_distribute_time = np.repeat(0,self.task.shape[0])\n",
    "        self.total_task_process_time = np.repeat(0,self.task.shape[0])\n",
    "        self.task_status = np.repeat(1,self.task.shape[0])  \n",
    "        self.task_index = list(range(self.task.shape[0]))  \n",
    "        self.timeindex = 0   \n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        self.done_task = [] \n",
    "        self.done_VM = [] \n",
    "        self.task_start_time = [] \n",
    "        self.state_dim = self.state.shape[0]\n",
    "        self.action_dim = 2\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.task_num = self.task.shape[0]\n",
    "        self.VM_status = np.repeat(0,self.VM) \n",
    "        self.VM_process_task = [[] for i in range(self.VM)]\n",
    "        self.VM_process_time = [[] for i in range(self.VM)]\n",
    "        self.task_waiting_time = [[] for i in range(self.VM)]\n",
    "        #self.left_task = self.task.shape[0]\n",
    "        self.done = False\n",
    "        self.total_time = 0  \n",
    "        self.task_distribute_time = np.repeat(0,self.task.shape[0])\n",
    "        self.total_task_process_time = np.repeat(0,self.task.shape[0])\n",
    "        self.task_status = np.repeat(1,self.task.shape[0])  \n",
    "        self.task_index = list(range(self.task.shape[0]))  \n",
    "        #self.timeindex = 0  \n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        self.done_task = []\n",
    "        self.done_VM = [] \n",
    "        self.task_start_time = [] \n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        task_id = np.random.choice(a=self.task_num, size=self.VM, replace=False, p=None)\n",
    "        for i in task_id:\n",
    "            if len(self.task_index) != 0:\n",
    "                if i in self.task_index:\n",
    "                    self.task_distribute_time[i] += 1\n",
    "                    ## if more than 2, delete this task\n",
    "                    #if self.task_distribute_time[i] >= 2:\n",
    "                    #    del self.task_index[self.task_index.index(i)]\n",
    "                else:\n",
    "                    task_id[task_id.tolist().index(i)] = random.sample(self.task_index,1)[0]\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        assert action.shape[0] == self.VM\n",
    "        \n",
    "        for i in range(self.VM):\n",
    "            ## only process those tasks that are in task_index\n",
    "            if task_id[i] in self.task_index:\n",
    "                ## action = 0 indicates do not give tasks to the VM\n",
    "                if action[i] == 0 or self.VM_status[i] == 3:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.VM_process_task[i].append(task_id[i])\n",
    "                    self.VM_status[i] += 1\n",
    "                    self.task_status[task_id[i]] = 0\n",
    "                    self.VM_process_time[i].append(0)\n",
    "                    # how much time a task wait before processing\n",
    "                    self.task_waiting_time[i].append(self.timeindex)\n",
    "                    # if VM could not handle the task, exit\n",
    "                    self.total_task_process_time[task_id[i]] = self.process_time[i][self.task[task_id[i]][2]]\n",
    "                \n",
    "                delete_index = []\n",
    "                for j in range(len(self.VM_process_time[i])):\n",
    "                    if len(self.VM_process_task[i]) != 0:\n",
    "                        if self.VM_process_time[i][j] == self.process_time[i][self.task[self.VM_process_task[i][j]][2]]:\n",
    "                            # if task finished, workload of VM would decrease\n",
    "                            self.VM_status[i] -= 1\n",
    "                            self.done_VM.append(i)\n",
    "                            if self.VM_process_task[i][j] not in self.done_task:\n",
    "                                self.left_task -= 1\n",
    "                            self.done_task.append(self.VM_process_task[i][j])\n",
    "                            ## calculate when the task starts to be processed by subtracting the process time\n",
    "                            self.task_start_time.append(self.task_waiting_time[i][j] + self.task[self.VM_process_task[i][j]][1])\n",
    "                            delete_index.append(j)\n",
    "                if len(delete_index) > 0:\n",
    "                    if len(delete_index) > 1:\n",
    "                        delete_index.sort(reverse = True)\n",
    "                    for k in delete_index:\n",
    "                        del self.VM_process_task[i][k]\n",
    "                        del self.VM_process_time[i][k]\n",
    "            ## calculate total time consumed\n",
    "            self.total_time += sum(self.task_waiting_time[i]) + self.total_task_process_time[i].sum()\n",
    "            self.VM_process_time[i] = [m + 1 for m in self.VM_process_time[i]]\n",
    "        \n",
    "        ## reward takes the minus of total time*0.001 and left task num\n",
    "        #print(self.total_time)\n",
    "        reward = 1 - self.left_task/self.task_num\n",
    "        self.timeindex += 1\n",
    "        \n",
    "        ## update state info\n",
    "        self.state = np.vstack((self.task_status,self.task_distribute_time))\n",
    "        self.state = self.state.reshape(self.state.shape[0],self.state.shape[1],1)\n",
    "        \n",
    "        if self.left_task == 0:\n",
    "            self.done = True\n",
    "        #print(self.VM_status)\n",
    "        #print(self.VM_process_task)\n",
    "        #print(self.done_task)\n",
    "        return self.state, reward, self.done, self.done_task, self.done_VM, self.task_start_time\n",
    "\n",
    "    def update(self,delete_list):\n",
    "        if len(delete_list) != 0:\n",
    "            for i in delete_list:\n",
    "                if i in self.task_index:\n",
    "                    self.task_index.remove(i)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(32*553, 256)\n",
    "\n",
    "        num_outputs = action_space\n",
    "        self.critic_linear = nn.Linear(256, 1)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = normalized_columns_initializer(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = normalized_columns_initializer(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        x = x.view(-1, 32*553)\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n",
    "    \n",
    "    def choose_action(self,inputs,action_dim):\n",
    "        s, (hx, cx) = inputs\n",
    "        value, logit, (hx, cx) = self.forward((s.unsqueeze(0),(hx, cx)))\n",
    "        prob = F.softmax(logit, dim=-1)\n",
    "        log_prob = F.log_softmax(logit, dim=-1)\n",
    "        entropy = -(log_prob * prob).sum(1, keepdim=True)\n",
    "        \n",
    "        #action = prob.multinomial(num_samples=action_dim).detach()\n",
    "        action=[]\n",
    "        for i in range(action_dim):\n",
    "            action.append(prob.multinomial(num_samples=1).detach()[0])\n",
    "        action = torch.from_numpy(np.array(action,dtype = np.int64).reshape(1,133))\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "\n",
    "    lr = 0.01\n",
    "    gamma = 0.9\n",
    "    gae_lambda = 1.00\n",
    "    entropy_coef = 0.01\n",
    "    value_loss_coef = 0.5\n",
    "    max_grad_norm = 50\n",
    "    seed= 2020\n",
    "    num_steps = 100\n",
    "    max_episode_length= 50\n",
    "    episode = 50\n",
    "\n",
    "    #torch.manual_seed(args.seed)\n",
    "\n",
    "    env = Job_VM_env()\n",
    "    \n",
    "    model = ActorCritic(env.state_dim, env.action_dim)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    state = env.reset()\n",
    "    state = v_wrap(state)\n",
    "    done = True\n",
    "    action_dim = env.VM\n",
    "\n",
    "    episode_length = 0\n",
    "    complete_tasks = []\n",
    "    VM_complete_task = []\n",
    "    complete_task_start_time = []\n",
    "    update_list = []\n",
    "    for episode in range(episode):\n",
    "        \n",
    "        if done:\n",
    "            cx = torch.zeros(1, 256)\n",
    "            hx = torch.zeros(1, 256)\n",
    "        else:\n",
    "            cx = cx.detach()\n",
    "            hx = hx.detach()\n",
    "        \n",
    "        if len(complete_tasks) != 0:\n",
    "            update_list = [n for m in complete_tasks for n in m]\n",
    "            env.update(update_list)\n",
    "\n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "\n",
    "        for step in range(num_steps+1):\n",
    "            episode_length += 1\n",
    "            \n",
    "\n",
    "            action, log_prob, entropy, value = model.choose_action((state, (hx,cx)),action_dim)\n",
    "            #print(\"choosen action:\", action, \"with Prob:\", log_prob, \"value:\", value)\n",
    "            log_prob = log_prob.gather(1, action)[0]\n",
    "            \n",
    "            state, reward, done, done_task, done_VM, task_start_time = env.step(action.view(-1,).numpy())\n",
    "            #print(\"state:\", state, \"reward at state action:\", reward, \"done_task\", done_task, \"done_VM\", done_VM)\n",
    "\n",
    "            done = done or episode_length >= max_episode_length\n",
    "            ## reward shaping\n",
    "            reward = np.round(max(min(reward, 1), -1),3)\n",
    "            if episode_length % 20 == 0:\n",
    "                print(\"reward:\", reward)\n",
    "                #print(done_task)\n",
    "\n",
    "            if done:\n",
    "                complete_tasks.append(done_task)\n",
    "                VM_complete_task.append(done_VM)\n",
    "                complete_task_start_time.append(task_start_time)\n",
    "                #print('Complete these tasks with 100 iterations:')\n",
    "                #print(complete_tasks)\n",
    "                print('Current episode:',episode)\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "                \n",
    "                final_output =[]\n",
    "                for i in range(len(complete_tasks)):\n",
    "                    for j in range(len(complete_tasks[i])):\n",
    "                        output = ([complete_tasks[i][j]+1, VM_complete_task[i][j]+1, complete_task_start_time[i][j]])\n",
    "                        final_output.append(output)\n",
    "\n",
    "                final_output1 = pd.DataFrame(final_output)\n",
    "                print(\"makespan:\",final_output1[2].sum())\n",
    "                final_output1.to_csv(\"output_file.csv\")\n",
    "\n",
    "\n",
    "            state = v_wrap(state)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(\"total_reward:\",rewards)\n",
    "\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n",
    "            R = value.detach()\n",
    "\n",
    "        values.append(R)\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            # Generalized Advantage Estimation\n",
    "            delta_t = rewards[i] + gamma * \\\n",
    "                values[i + 1] - values[i]\n",
    "            gae = gae * gamma * gae_lambda + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                log_probs[i] * gae.detach() - entropy_coef * entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (policy_loss + value_loss_coef * value_loss).backward(torch.ones_like(policy_loss))\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        print(\"total loss (policy+value):\", policy_loss.mean() + value_loss_coef * value_loss)\n",
    "        print('parameters of model updated')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.001\n",
      "reward: 0.004\n",
      "Current episode: 0\n",
      "makespan: 31941\n",
      "total_reward: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.004, 0.005]\n",
      "total loss (policy+value): tensor([[2.2853]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.006\n",
      "reward: 0.007\n",
      "Current episode: 1\n",
      "makespan: 60888\n",
      "total_reward: [0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.005, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.007, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008]\n",
      "total loss (policy+value): tensor([[80.1137]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.01\n",
      "reward: 0.012\n",
      "Current episode: 2\n",
      "makespan: 100189\n",
      "total_reward: [0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.008, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.009, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.011, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.012, 0.013, 0.013, 0.013, 0.013, 0.013]\n",
      "total loss (policy+value): tensor([[-22.1207]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.014\n",
      "reward: 0.016\n",
      "Current episode: 3\n",
      "makespan: 132964\n",
      "total_reward: [0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.013, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.014, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.016, 0.017, 0.017, 0.017, 0.017]\n",
      "total loss (policy+value): tensor([[22.0323]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.017\n",
      "reward: 0.019\n",
      "Current episode: 4\n",
      "makespan: 160954\n",
      "total_reward: [0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.017, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.018, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.019, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]\n",
      "total loss (policy+value): tensor([[2.7917]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.021\n",
      "reward: 0.024\n",
      "Current episode: 5\n",
      "makespan: 204127\n",
      "total_reward: [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.021, 0.021, 0.021, 0.021, 0.021, 0.021, 0.022, 0.022, 0.022, 0.022, 0.022, 0.022, 0.022, 0.022, 0.022, 0.023, 0.023, 0.023, 0.023, 0.023, 0.023, 0.023, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024]\n",
      "total loss (policy+value): tensor([[-6.3631]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.026\n",
      "reward: 0.028\n",
      "Current episode: 6\n",
      "makespan: 245527\n",
      "total_reward: [0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.024, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.026, 0.027, 0.027, 0.027, 0.027, 0.027, 0.027, 0.027, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028]\n",
      "total loss (policy+value): tensor([[2.1944]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.03\n",
      "reward: 0.031\n",
      "Current episode: 7\n",
      "makespan: 288716\n",
      "total_reward: [0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.028, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.029, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.031, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032]\n",
      "total loss (policy+value): tensor([[2.1316]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.033\n",
      "reward: 0.036\n",
      "Current episode: 8\n",
      "makespan: 336229\n",
      "total_reward: [0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.032, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.033, 0.034, 0.034, 0.034, 0.034, 0.034, 0.034, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.035, 0.036, 0.036, 0.036, 0.036, 0.036, 0.036, 0.036, 0.036, 0.036, 0.037, 0.037]\n",
      "total loss (policy+value): tensor([[0.1299]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.038\n",
      "reward: 0.039\n",
      "Current episode: 9\n",
      "makespan: 380635\n",
      "total_reward: [0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.037, 0.038, 0.038, 0.038, 0.038, 0.038, 0.038, 0.038, 0.038, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.039, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.041]\n",
      "total loss (policy+value): tensor([[-2.5245]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.042\n",
      "reward: 0.044\n",
      "Current episode: 10\n",
      "makespan: 429959\n",
      "total_reward: [0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.041, 0.042, 0.042, 0.042, 0.042, 0.042, 0.042, 0.042, 0.042, 0.042, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.044, 0.045, 0.045, 0.045, 0.045]\n",
      "total loss (policy+value): tensor([[1.4323]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.045\n",
      "reward: 0.047\n",
      "Current episode: 11\n",
      "makespan: 465195\n",
      "total_reward: [0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.045, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.046, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.047, 0.048]\n",
      "total loss (policy+value): tensor([[0.9611]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.049\n",
      "reward: 0.051\n",
      "Current episode: 12\n",
      "makespan: 519214\n",
      "total_reward: [0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.051, 0.052, 0.052, 0.052]\n",
      "total loss (policy+value): tensor([[-0.4657]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.053\n",
      "reward: 0.055\n",
      "Current episode: 13\n",
      "makespan: 578404\n",
      "total_reward: [0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.052, 0.053, 0.053, 0.053, 0.053, 0.053, 0.053, 0.053, 0.054, 0.054, 0.054, 0.054, 0.054, 0.054, 0.054, 0.054, 0.054, 0.055, 0.055, 0.055, 0.055, 0.055, 0.055, 0.055, 0.055, 0.055, 0.055, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056]\n",
      "total loss (policy+value): tensor([[1.1249]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.057\n",
      "reward: 0.059\n",
      "Current episode: 14\n",
      "makespan: 628372\n",
      "total_reward: [0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.056, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.057, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.058, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.059, 0.06, 0.06, 0.06, 0.06]\n",
      "total loss (policy+value): tensor([[1.2400]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.061\n",
      "reward: 0.063\n",
      "Current episode: 15\n",
      "makespan: 683084\n",
      "total_reward: [0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.06, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.061, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.063, 0.063, 0.063, 0.063, 0.063, 0.063, 0.063, 0.063, 0.063, 0.063, 0.064, 0.064]\n",
      "total loss (policy+value): tensor([[-0.8401]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.065\n",
      "reward: 0.067\n",
      "Current episode: 16\n",
      "makespan: 731676\n",
      "total_reward: [0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.064, 0.065, 0.065, 0.065, 0.065, 0.065, 0.065, 0.065, 0.065, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067]\n",
      "total loss (policy+value): tensor([[2.5032]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.068\n",
      "reward: 0.07\n",
      "Current episode: 17\n",
      "makespan: 781285\n",
      "total_reward: [0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.067, 0.068, 0.068, 0.068, 0.068, 0.068, 0.068, 0.068, 0.068, 0.068, 0.068, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.069, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07]\n",
      "total loss (policy+value): tensor([[1.9661]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.072\n",
      "reward: 0.073\n",
      "Current episode: 18\n",
      "makespan: 837559\n",
      "total_reward: [0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.071, 0.071, 0.071, 0.071, 0.071, 0.071, 0.071, 0.071, 0.071, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.072, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.073, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074]\n",
      "total loss (policy+value): tensor([[-2.3239]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.076\n",
      "reward: 0.078\n",
      "Current episode: 19\n",
      "makespan: 921400\n",
      "total_reward: [0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.074, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.076, 0.076, 0.076, 0.076, 0.076, 0.076, 0.076, 0.076, 0.077, 0.077, 0.077, 0.077, 0.077, 0.077, 0.077, 0.077, 0.077, 0.077, 0.078, 0.078, 0.078, 0.078, 0.078, 0.078, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079]\n",
      "total loss (policy+value): tensor([[-2.0915]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.08\n",
      "reward: 0.082\n",
      "Current episode: 20\n",
      "makespan: 980995\n",
      "total_reward: [0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.079, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.081, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082]\n",
      "total loss (policy+value): tensor([[1.8274]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.084\n",
      "reward: 0.086\n",
      "Current episode: 21\n",
      "makespan: 1049006\n",
      "total_reward: [0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.082, 0.083, 0.083, 0.083, 0.083, 0.083, 0.083, 0.084, 0.084, 0.084, 0.084, 0.084, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.085, 0.086, 0.086, 0.086, 0.086, 0.086, 0.086, 0.086, 0.086, 0.086, 0.086, 0.087, 0.087, 0.087]\n",
      "total loss (policy+value): tensor([[0.6501]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.088\n",
      "reward: 0.09\n",
      "Current episode: 22\n",
      "makespan: 1127945\n",
      "total_reward: [0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.087, 0.088, 0.088, 0.088, 0.088, 0.088, 0.088, 0.088, 0.088, 0.088, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.089, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091]\n",
      "total loss (policy+value): tensor([[-4.2784]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.092\n",
      "reward: 0.094\n",
      "Current episode: 23\n",
      "makespan: 1188405\n",
      "total_reward: [0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.091, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.092, 0.093, 0.093, 0.093, 0.093, 0.093, 0.093, 0.093, 0.093, 0.093, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094]\n",
      "total loss (policy+value): tensor([[-4.9644]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.095\n",
      "reward: 0.097\n",
      "Current episode: 24\n",
      "makespan: 1237723\n",
      "total_reward: [0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.094, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.095, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.096, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097]\n",
      "total loss (policy+value): tensor([[-2.1469]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.099\n",
      "reward: 0.101\n",
      "Current episode: 25\n",
      "makespan: 1320164\n",
      "total_reward: [0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.097, 0.098, 0.098, 0.098, 0.098, 0.098, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.099, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.101]\n",
      "total loss (policy+value): tensor([[5.1621]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.103\n",
      "reward: 0.103\n",
      "Current episode: 26\n",
      "makespan: 1370678\n",
      "total_reward: [0.101, 0.101, 0.101, 0.101, 0.101, 0.101, 0.102, 0.102, 0.102, 0.102, 0.102, 0.102, 0.102, 0.102, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.103, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104]\n",
      "total loss (policy+value): tensor([[7.9291]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.105\n",
      "reward: 0.107\n",
      "Current episode: 27\n",
      "makespan: 1457111\n",
      "total_reward: [0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.105, 0.105, 0.105, 0.105, 0.105, 0.105, 0.105, 0.105, 0.105, 0.105, 0.106, 0.106, 0.106, 0.106, 0.106, 0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107, 0.107, 0.107, 0.107, 0.107, 0.107, 0.108, 0.108, 0.108, 0.108, 0.108, 0.108, 0.108, 0.109, 0.109]\n",
      "total loss (policy+value): tensor([[7.5549]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.109\n",
      "reward: 0.111\n",
      "Current episode: 28\n",
      "makespan: 1515656\n",
      "total_reward: [0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.109, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.111, 0.112]\n",
      "total loss (policy+value): tensor([[4.0134]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.114\n",
      "reward: 0.116\n",
      "Current episode: 29\n",
      "makespan: 1630073\n",
      "total_reward: [0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.112, 0.113, 0.113, 0.113, 0.114, 0.114, 0.114, 0.114, 0.114, 0.114, 0.114, 0.115, 0.115, 0.115, 0.115, 0.115, 0.115, 0.115, 0.115, 0.115, 0.115, 0.116, 0.116, 0.116, 0.116, 0.116, 0.116, 0.116, 0.116, 0.116, 0.116, 0.117, 0.117, 0.117, 0.117, 0.117, 0.117]\n",
      "total loss (policy+value): tensor([[-1.0671]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.119\n",
      "reward: 0.121\n",
      "Current episode: 30\n",
      "makespan: 1718472\n",
      "total_reward: [0.117, 0.117, 0.117, 0.117, 0.117, 0.117, 0.117, 0.117, 0.118, 0.118, 0.118, 0.118, 0.118, 0.118, 0.118, 0.119, 0.119, 0.119, 0.119, 0.119, 0.119, 0.119, 0.119, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121]\n",
      "total loss (policy+value): tensor([[-1.5586]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.123\n",
      "reward: 0.125\n",
      "Current episode: 31\n",
      "makespan: 1807051\n",
      "total_reward: [0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.121, 0.122, 0.122, 0.122, 0.122, 0.122, 0.122, 0.122, 0.122, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.123, 0.124, 0.124, 0.124, 0.124, 0.124, 0.124, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.126, 0.126, 0.126]\n",
      "total loss (policy+value): tensor([[0.9249]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.127\n",
      "reward: 0.129\n",
      "Current episode: 32\n",
      "makespan: 1897878\n",
      "total_reward: [0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.126, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.127, 0.128, 0.128, 0.128, 0.128, 0.128, 0.128, 0.128, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.129, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13]\n",
      "total loss (policy+value): tensor([[3.4015]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.131\n",
      "reward: 0.134\n",
      "Current episode: 33\n",
      "makespan: 2009564\n",
      "total_reward: [0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.13, 0.131, 0.131, 0.131, 0.131, 0.131, 0.131, 0.131, 0.132, 0.132, 0.132, 0.132, 0.132, 0.132, 0.133, 0.133, 0.133, 0.133, 0.133, 0.133, 0.133, 0.133, 0.133, 0.133, 0.134, 0.134, 0.134, 0.134, 0.134, 0.134, 0.134, 0.134, 0.134, 0.135, 0.135, 0.135, 0.135, 0.135]\n",
      "total loss (policy+value): tensor([[2.8492]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.136\n",
      "reward: 0.136\n",
      "Current episode: 34\n",
      "makespan: 2062125\n",
      "total_reward: [0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.135, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.136, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137]\n",
      "total loss (policy+value): tensor([[-1.9786]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.138\n",
      "reward: 0.139\n",
      "Current episode: 35\n",
      "makespan: 2109124\n",
      "total_reward: [0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.137, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.138, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139]\n",
      "total loss (policy+value): tensor([[-2.3188]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.14\n",
      "reward: 0.142\n",
      "Current episode: 36\n",
      "makespan: 2196447\n",
      "total_reward: [0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.139, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.141, 0.141, 0.141, 0.141, 0.141, 0.141, 0.141, 0.141, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.142, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143]\n",
      "total loss (policy+value): tensor([[-3.4269]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.145\n",
      "reward: 0.146\n",
      "Current episode: 37\n",
      "makespan: 2294136\n",
      "total_reward: [0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.143, 0.144, 0.144, 0.144, 0.144, 0.144, 0.144, 0.144, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.145, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.146, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147]\n",
      "total loss (policy+value): tensor([[-1.8226]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.148\n",
      "reward: 0.15\n",
      "Current episode: 38\n",
      "makespan: 2388232\n",
      "total_reward: [0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.147, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.148, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.149, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.151, 0.151, 0.151, 0.151]\n",
      "total loss (policy+value): tensor([[1.5984]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.151\n",
      "reward: 0.154\n",
      "Current episode: 39\n",
      "makespan: 2486568\n",
      "total_reward: [0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.151, 0.152, 0.152, 0.152, 0.152, 0.152, 0.152, 0.152, 0.152, 0.152, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.153, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.154, 0.155, 0.155]\n",
      "total loss (policy+value): tensor([[-0.6038]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.156\n",
      "reward: 0.158\n",
      "Current episode: 40\n",
      "makespan: 2591923\n",
      "total_reward: [0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.155, 0.156, 0.156, 0.156, 0.156, 0.156, 0.156, 0.156, 0.156, 0.156, 0.156, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.157, 0.158, 0.158, 0.158, 0.158, 0.158, 0.158, 0.159, 0.159, 0.159, 0.159, 0.159, 0.159]\n",
      "total loss (policy+value): tensor([[-3.9406]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.16\n",
      "reward: 0.163\n",
      "Current episode: 41\n",
      "makespan: 2704865\n",
      "total_reward: [0.159, 0.159, 0.159, 0.159, 0.159, 0.159, 0.159, 0.159, 0.159, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.16, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.161, 0.162, 0.162, 0.162, 0.162, 0.162, 0.162, 0.162, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163]\n",
      "total loss (policy+value): tensor([[-1.6182]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.165\n",
      "reward: 0.166\n",
      "Current episode: 42\n",
      "makespan: 2811124\n",
      "total_reward: [0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.163, 0.164, 0.164, 0.164, 0.164, 0.164, 0.164, 0.164, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.165, 0.166, 0.166, 0.166, 0.166, 0.166, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167]\n",
      "total loss (policy+value): tensor([[2.8650]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.168\n",
      "reward: 0.17\n",
      "Current episode: 43\n",
      "makespan: 2893269\n",
      "total_reward: [0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.167, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.168, 0.169, 0.169, 0.169, 0.169, 0.169, 0.169, 0.169, 0.169, 0.169, 0.169, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17, 0.17]\n",
      "total loss (policy+value): tensor([[3.3489]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.172\n",
      "reward: 0.174\n",
      "Current episode: 44\n",
      "makespan: 2996907\n",
      "total_reward: [0.17, 0.17, 0.17, 0.17, 0.17, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.171, 0.172, 0.172, 0.172, 0.172, 0.172, 0.172, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.173, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174]\n",
      "total loss (policy+value): tensor([[-0.4112]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.175\n",
      "reward: 0.177\n",
      "Current episode: 45\n",
      "makespan: 3103385\n",
      "total_reward: [0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.174, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.175, 0.176, 0.176, 0.176, 0.176, 0.176, 0.176, 0.176, 0.176, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.177, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178]\n",
      "total loss (policy+value): tensor([[-1.0278]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.179\n",
      "reward: 0.181\n",
      "Current episode: 46\n",
      "makespan: 3208378\n",
      "total_reward: [0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.178, 0.179, 0.179, 0.179, 0.179, 0.179, 0.179, 0.179, 0.179, 0.179, 0.179, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.18, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.181, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182]\n",
      "total loss (policy+value): tensor([[2.0722]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.183\n",
      "reward: 0.185\n",
      "Current episode: 47\n",
      "makespan: 3315681\n",
      "total_reward: [0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.182, 0.183, 0.183, 0.183, 0.183, 0.183, 0.183, 0.183, 0.183, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.184, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.185, 0.186, 0.186, 0.186, 0.186]\n",
      "total loss (policy+value): tensor([[0.5320]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.187\n",
      "reward: 0.189\n",
      "Current episode: 48\n",
      "makespan: 3415564\n",
      "total_reward: [0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.186, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.187, 0.188, 0.188, 0.188, 0.188, 0.188, 0.188, 0.188, 0.188, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189]\n",
      "total loss (policy+value): tensor([[-2.5084]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n",
      "reward: 0.191\n",
      "reward: 0.192\n",
      "Current episode: 49\n",
      "makespan: 3525665\n",
      "total_reward: [0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.189, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.191, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.192, 0.193, 0.193, 0.193, 0.193, 0.193, 0.193]\n",
      "total loss (policy+value): tensor([[0.5206]], grad_fn=<AddBackward0>)\n",
      "parameters of model updated\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
